\section
[$M/G/1$ Queue Length Distribution]
{$\mathbf{M/G/1}$ Queue Length Distribution}
\label{sec:distr-queue-length}


\subsection*{Theory and Exercises}

\Opensolutionfile{hint}
\Opensolutionfile{ans}


In Section~\ref{sec:batch-arrivals} we used level-crossing arguments 
to find a recursive method to compute  the stationary distribution $p(n)$ of the number of items in an $M^X/M/1$ queue. Here we apply similar arguments to find $p(n)$ for the $M/G/1$ queue.
However, we cannot simply copy the derivation of the $M^X/M/1$ queue to the $M/G/1$ queue, because in the $M^X/M/1$ queue the item service times are exponential,
hence memoryless, while in the $M/G/1$ this is not the case. When job service times are 
not memoryless, hence do not restart at arrival times, we cannot choose any moment we like to apply level-crossing. Thus, for the $M/G/1$ queue
we need to focus on moments in time in which the system `restarts', which as we will see are job departure epochs.  The argumentation is quite subtle, as it uses an
interplay of the PASTA property and relation~\eqref{eq:39}
between $\pi(n)$, $p(n)$ and $\delta(n)$.  

An important role below is played by  the number of  arrivals $Y_k$ during the service time of the $k$th job.  Since the service times of the jobs for an i.i.d. sequence of random variables, the sequence $\{Y_k\}$ is also i.i.d. Let $Y$ be the common random variable with probability mass
 $f(j) = \P{Y = j}$ ; write $G(j) = \P{Y_k > j}$ for the survivor function.


\begin{exercise}
 Explain that if the service time is constant and equal to $s$, then
\begin{equation}\label{eq:4}
  \P{Y_k = j\given S=s} = e^{-\lambda s}\frac{(\lambda s)^j}{j!}.
\end{equation}
\begin{hint}
If $s$ is deterministic, the number of arrival during a fixed
    period of time with length $s$ must be Poisson distributed.
\end{hint}
  \begin{solution}
See the hint.  The period during which the arrivals occur is $s$. 
  \end{solution}
\end{exercise}

\begin{exercise}
 Explain that 
\begin{equation}\label{eq:10}
  \P{Y_k = j} = \int_0^\infty e^{-\lambda x}\frac{(\lambda x)^j}{j!}\, \d F(x).
\end{equation}
where $F$ is the distribution of the service times.
\begin{solution}
  We use a conditioning argument to arrive at this result. The
  probability that the service time is $x$ units long is written in
  various ways in the literature: $F(\d x) = \d F(x) = \P{S\in \d x}$,
  but this all means the same thing, it is only the notation that
  differs.  (As an aside, to properly define this we need measure
  theory). When $F$ has a density $f$, then
  $\d F(x) = f(x) \d x$.  Note that when $S$ is discrete, it does not
  have a density everywhere. With this,
    \begin{equation*}
    \P{Y_k=j} = \int_0^\infty \P{Y_k =j\given S=x}\P{S\in \d x} =
    \int_0^\infty \P{Y_k =j\given S=x} \d F(x).
    \end{equation*}
    Using the answer of the previous problem we arrive at the result.
\end{solution}
\end{exercise}

\begin{exercise}
  If $S$ is deterministic and equal to $s$, show that
  Eq.~\eqref{eq:10} reduces to Eq.~\eqref{eq:4}.  
  \begin{hint}
 $S=s$ then
    $\P{S=s}=1$, i.e., all probability mass lies at $s$. Thus, all
    arrivals must occur during $[0,s]$.
  \end{hint}
  \begin{solution}
    Let us first attack this problem from a general point of
    view. Suppose the service time $S$ can take values
    $s_1<s_2<\cdots <s_n$, and $\P{S=s_i}=\alpha_i$. Then of course we
    want that $\P{S\leq s_n} = \sum_{i=1}^n \alpha_i = 1$. The
    distribution function $F$ of $S$ is in this case a step function,
    with steps at the points $s_1, s_2, \ldots$, and step sizes
    $\alpha_1, \alpha_2, \ldots$. Thus, $F(s_i)-F(s_i-)=\alpha_i$. We
    say that such a distribution function has \emph{atoms} at the
    points $s_1, s_2, \ldots$. In this case we write
  \begin{equation*}
    \int_0^\infty g(x) \d F(x) = \sum_{i=1}^n g(s_i) \alpha_i.
  \end{equation*}
  Thus, the integral of $g$ with respect to $F$ is the sum of $g$ at
  the points at which $F$ makes a jump times the weight of $F$ at
  these points. 

  As an example, in case $S\equiv 10$ (the service time is always 10
  time units long) the distribution function $F$ makes just one jump
  at $s_1=10$ of size $\alpha_1 = F(10)-F(10-) =1$, i.e., $F$ has the
  form
    \begin{equation*}
      F(x) = 
      \begin{cases}
        0, &\quad x< 10, \\
        1, &\quad x\geq 10.
      \end{cases}
    \end{equation*}
With this, 
\begin{equation*}
  \int_0^\infty g(x) \d F(x) = \sum_{i=1}^n g(s_i) \alpha_i = g(s_1) \alpha_1 = g(10) \cdot 1 = g(10).
\end{equation*}
Thus, the integral of $g$ with respect to this distribution $F$ is
$g(10)$.  More generally, when $F$ puts all probability mass at the
single point $s$ (rather than at the point $10$), then
\begin{equation*}
  \int_0^\infty g(x) \d F(x) = g(s).
\end{equation*}

    Let us now copy the formula of the previous problem:
    \begin{equation*}
    \P{Y_k=j} = \int_0^\infty \P{Y_k =j\given S=x} \d F(x).
    \end{equation*}
    We see that here the integrand is the function
    $g(x) = \P{Y_k=j\given S=x}$. It is given in the question that $F$
    puts all mass on the point $s$. Therefore,
    \begin{equation*}
    \P{Y_k=j} = \int_0^\infty \P{Y_k =j\given S=x} \d F(x) = 
\int_0^\infty g(x) \d F(x) = g(s) \qquad \star
    \end{equation*}

    Now we also know that if the service time takes precisely $x$ time
    units, the number of arrivals is Poisson distributed. Therefore
    \begin{equation*}
    \P{Y_k =j\given S=x} = e^{-\lambda x}\frac{(\lambda x)^j}{j!}.
    \end{equation*}
Hence, 
    \begin{equation*}
    g(x) = \P{Y_k =j\given S=x} = e^{-\lambda x}\frac{(\lambda x)^j}{j!}.
    \end{equation*}


    Finally, using $(\star)$, and taking $x=s$ in the above expression of $g$, we get
    \begin{equation*}
    \P{Y_k=j} = \int_0^\infty \P{Y_k =j\given S=x} \d F(x) = 
    g(s) = e^{-\lambda s}\frac{(\lambda s)^j}{j!}.
    \end{equation*}
  \end{solution}
\end{exercise}

\begin{exercise}
 If $S\sim \exp(\mu)$, show that 
  \begin{equation} \label{eq:41}
f(j) = \P{Y_k = j} = \frac{\mu}{\lambda+\mu}\left(\frac{\lambda}{\lambda+\mu}\right)^j.
  \end{equation}
  \begin{solution}
Use conditional probability to see that 
\begin{equation*}
  \begin{split}
  \P{Y_n = j} 
&= \int_0^\infty e^{-\lambda x}\frac{(\lambda x)^j}{j!}\, \d F(x) = \int_0^\infty e^{-\lambda x}\frac{(\lambda x)^j}{j!} \mu e^{-\mu x}\, \d x \\
&= \frac{\mu}{j!}\lambda^j \int_0^\infty e^{-(\lambda+\mu) x}x^j\,\d x = \frac{\mu}{j!}\left(\frac{\lambda}{\lambda+\mu}\right)^j \int_0^\infty e^{-(\lambda+\mu) x}((\lambda+\mu)x)^j\,\d x \\
&= \frac{\mu}{j!}\left(\frac{\lambda}{\lambda+\mu}\right)^j \frac{j!}{\lambda+\mu}.
\end{split}
\end{equation*}
Here I also used the hints and  results of Exercise~\ref{ex:33}.

In hindsight, this result could have been derived in another way, in
fact by using the result of Exercise~\ref{ex:3} in which we analyzed a
merged Poisson process. Consider the Poisson process with rate
$\lambda+\mu$ that arises when the arrival and service process are
merged. The probability that an arrival corresponds to an epoch of the
merged process is $\lambda/(\lambda+\mu$ and the probability that a
departure corresponds to an epoch of the merged process is
$\mu/(\lambda+\mu)$.  The probability that $j$ arrivals occur before a
service occurs, is the same as the probability that a geometrically
distributed random variable with success probability
$\mu/(\lambda+\mu) = 1-p$ takes the value $j$.
  \end{solution}
\end{exercise}

\begin{exercise}
 If $S\sim \exp(\mu)$, show that 
  \begin{equation}
G(j) = \sum_{k=j+1}^\infty f(k) =  \left(\frac{\lambda}{\lambda+\mu}\right)^{j+1}.
  \end{equation}
\begin{solution}
  Take $\alpha = \lambda/(\lambda+\mu)$ so that
  $f(j) = (1-\alpha) \alpha^j$.
\begin{equation*}
  \begin{split}
  G(j) 
&= \sum_{k=j+1}^\infty f(k)  = (1-\alpha) \sum_{k=j+1}^\infty \alpha^k \\
& = (1-\alpha) \sum_{k=0}^\infty \alpha^{k+j+1}, \text{ by change of variable}\\
& = (1-\alpha) \sum_{k=0}^\infty \alpha^{k}\alpha^{j+1}= (1-\alpha)\alpha^{j+1} \sum_{k=0}^\infty \alpha^k \\
&= (1-\alpha)\alpha^{j+1} \frac{1}{1-\alpha} = \alpha^{j+1}.
  \end{split}
\end{equation*}
    \end{solution}
\end{exercise}

Let us concentrate on a downcrossing of level $n$, see
Figure~\ref{fig:mg1_2}; recall that level $n$ lies between states $n$
and $n+1$. For job $k$ to generate a downcrossing of level $n$, two events must take place: 
 job `$k-1$' must leave $n+1$ jobs behind after its service completion, and  job $k$ must leave $n$ jobs behind. Thus, 
 \begin{equation*}
   \text{Downcrossing of level $n$} \iff \1{L(D_{k-1}) = n+1}\1{L(D_k)=n} = 1.
 \end{equation*}
Let us write this in another
way. Observe that if $L(D_{k-1})=n+1$ and no other jobs arrive during
the service time $S_k$ of job $k$, i.e., when $Y_k=0$, it must also be
that job $k$ leaves $n$ jobs behind. If, however, $Y_k>0$, then
$L(D_k)\geq n+1$.  Thus, we see that
 \begin{equation*}
   \text{Downcrossing of level $n$} \iff \1{L(D_{k-1}) = n+1}\1{Y_k=0} = 1.
 \end{equation*}
% \begin{equation*}
%   \1{L(D_{k-1}) = n+1}\1{L(D_k)=n} =  \1{L(D_{k-1}) = n+1}\1{Y_k=0}.
% \end{equation*}
Consequently, the number of downcrossings of level $n$ up to time $t$ is
\begin{equation*}
  \begin{split}
  D(n+1, 0, t) 
%&= \sum_{k=1}^\infty \1{D_{k}\leq t}\1{L(D_{k})=n} \\
&= \sum_{k=1}^{D(t)}\1{L(D_{k-1})=n+1}\1{Y_k=0}.
  \end{split}
\end{equation*}

\begin{exercise}
Use a similar derivation as in~\eqref{eq:1332} to   show that 
\begin{equation*}
  \lim_{t\to\infty} \frac{  D(n+1, 0, t)}t  = \gamma \delta(n+1) f(0),
\end{equation*}
where $f(0)=\P{Y=0}$.
\begin{solution}
By using the definitions and limits developed in
Sections~\ref{sec:rate-stability} and~\ref{sec:poisson-arrivals-see},
\begin{equation*}
  \begin{split}
  \lim_{t\to\infty} \frac{  D(n+1, 0, t)}t 
&=   \lim_{t\to\infty}  \frac{D(t)}t \frac{D(n+1, t)}{D(t)}\frac{ D(n+1, 0, t)}{D(n+1,t)} \\
&=   \gamma \delta(n+1) \lim_{t\to\infty} \frac{ D(n+1, 0, t)}{D(n+1,t)} \\
&=   \gamma \delta(n+1) \P{Y=0}\\
& = \gamma \delta(n+1) f(0),
  \end{split}
\end{equation*}
where the last limit follows from the independence of $Y_k$ and
$L(D_{k-1})$, 
\end{solution}
\end{exercise}

\begin{figure}[tb]
  \centering
                  
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,
                    semithick]
\node[state] (0) {$\delta(0)$}; 
\node[state] (1) [right of=0] {$\delta(1)$}; 
\node[state] (2) [right of=1] {$\delta(2)$}; 
\node[state] (3) [right of=2] {$\delta(3)$}; 
\node[state] (4) [right of=3] {$\delta(4)$}; 
%\node[state] (5) [right of=4] {$\cdots$}; 
\node (5) [above of=4] {};

\path 
(0) edge [bend left] node[above, very near start, fill=white] {$G(3)$} (5)
(0) edge [loop below] node[below, midway, fill=white] {$f(0)$} (0)
(1) edge [bend left] node[above, very near start, fill=white] {$G(3)$} (5)
(1) edge [loop below] node[below, midway, fill=white] {$f(1)$} (1)
(1) edge [bend left] node[below, midway, fill=white] {$f(0)$} (0)
(2) edge [bend left] node[above, very near start, fill=white] {$G(2)$} (5)
(2) edge [bend left] node[below, midway, fill=white] {$f(0)$} (1)
(2) edge [loop below] node[below, midway, fill=white] {$f(1)$} (2)
(3) edge [bend left] node[above, very near start, fill=white] {$G(1)$} (5)
(3) edge [loop below] node[below, midway, fill=white] {$f(1)$} (3)
(3) edge [bend left] node[below, midway, fill=white] {$f(0)$} (2)
(4) edge [bend left] node[below, near start] {$f(0)$} (3);

% \node[circ, right=of n-2] (n-1) {$n-1$}
% edge[loop below, thick]  node[midway, fill=white] {$\lambda f(0)$} (n-1); 

\draw[-, dotted, gray] (9.7,-1)--(9.7, 4.0) node[below, black] {level $3$};


\end{tikzpicture}
\caption{Level $3$ is crossed from below with rate
  $\gamma\delta(0)G(3) + \gamma\delta(1)G(3) + \cdots \gamma \delta(3) G(1)$ and crossed
  from above with rate $\gamma\delta(4) f(0)$. }
\label{fig:mg1_2}
\end{figure}

Before we deal with the upcrossing, it is important to do the next exercise.
\begin{exercise} Suppose that $L(D_{k-1})>0$. Why is
  $S_k = D_k - D_{k-1}$?  However, if $L(D_{k-1}) = 0$, the time between
  $D_{k-1}$ and $D_k$ is \emph{not} equal to $S_k$. Why not and what the
  distribution of $D_k-D_{k-1}$?
  \begin{hint}
Realize that if $L(D_{k-1})=0$, job $k-1$ leaves behind an empty
  system. Thus, before job $k$ can leave, it has to arrive. In other
  words, $D_{k-1}<A_k$.  Since job $k$ arrives to an empty system,
  his service starts right away, to that the time between $A_k$ and
  $D_k$ is equal to the service time of job $k$.
  \end{hint}
\begin{solution}
  When $L(D_{k-1})>0$, job $k$ is already in the system when job $k-1$
  finishes its service and leaves. Thus, at the departure time
  $D_{k-1}$ of job $k-1$, the service of job $k$ can start right away
  at $D_{k-1}$. Then, $D_k=D_{k-1}+S_k$.


    When job $k-1$ leaves an empty system behind, $D_k= A_k + S_k$,
    since job $k$ sees an empty system, hence its service can start
    right away after its arrival time at time $A_k$. Since the arrival
    process is Poisson by assumption, the time to the next arrival
    after $D_{k-1}$ is exponentially distributed with rate
    $\lambda$. (Recall the memoryless property of the interarrival
    times.) Thus, $A_k - D_{k-1}$ has the same distribution as $X_k$,
    so that $\P{D_k - D_{k-1} \leq x} = \P{X_k + S_k\leq x}$. BTW: Can
    you simplify this expression by using conditioning on $X_k$?
\end{solution}
\end{exercise}

For the upcrossings, assuming that $L(D_{k-1})=n>0$, then an upcrossing of level $n>0$  must have occurred when $L(D_k)>n$, i.e, 
 \begin{equation*}
 \1{L(D_{k-1}) = n}\1{L_k>n} = 1 \implies   \text{Upcrossing of level $n$}.
 \end{equation*}
Again, we can convert this into a statement about  the number of
arrivals $Y_k$ that occurred during the service time $S_k$ of job $k$.  If $Y_k=0$, then
job $k$ must leave $n-1$ jobs behind, so no upcrossing can
happen. Next, if $Y_k=1$, then job $k$ leaves $n$ jobs behind, so
still no upcrossing occurs. In fact, level $n$ is upcrossed from level $n$ can only occur if
more than one job arrives during the service of job $k$, i.e.,
\begin{equation*}
\1{L(D_{k-1})=n} \1{Y_{k}>1}  \implies   \text{Upcrossing of level $n$}.
\end{equation*}
More generally, level $n$ is upcrossed from level $m$, $0<m\leq n$ whenever
\begin{equation*}
\1{L(D_{k-1})=m} \1{Y_{k}>n-m+1}  \implies   \text{Upcrossing of level $n$}.
\end{equation*}
However,  if $m=0$ (think about this),
\begin{equation*}
\1{L(D_{k})>n} = \1{L(D_{k-1})=0} \1{Y_{k}>n}   \implies   \text{Upcrossing of level $n$}.
\end{equation*}

Again we define proper counting functions,  divide by $t$, and take suitable limits to find for upcrossing rate
\begin{equation}\label{eq:555}
\gamma \delta(0) G(n) + \gamma \sum_{m=1}^n \delta(m) G(n-m+1).
\end{equation}

\begin{exercise}
Provide the details behind the derivation of Eq.~\ref{eq:555}.
\begin{hint}
Define for $m=1,\ldots, n$
\begin{equation*}
  D(m, n, t) = \sum_{k=1}^{D(t)}\1{L(D_{k-1})=m}\1{Y_k>n-m+1},
\end{equation*}
and 
\begin{equation*}
  D(0, n, t) = \sum_{k=1}^{D(t)}\1{L(D_{k-1})=0}\1{Y_k>n}.
\end{equation*}
Then, divide by $D(n,t)$ and $D(t)$ and take limits.
\end{hint}
\begin{solution}
With the definition of the hint, 
\begin{equation*}
  \begin{split}
  \lim_{t\to\infty} \frac{  D(0, n, t)}t 
&=   \lim_{t\to\infty}  \frac{D(t)}t \frac{D(0, t)}{D(t)}\frac{ D(0, n, t)}{D(0,t)} \\
&=   \gamma \delta(0) \lim_{t\to\infty} \frac{ D(0, n, t)}{D(0,t)} \\
&=   \gamma \delta(0) \P{Y>n}\\
& = \gamma \delta(0) G(n).
  \end{split}
\end{equation*}

\begin{equation*}
  \begin{split}
  \lim_{t\to\infty} \frac{  D(m, n, t)}t 
&=   \lim_{t\to\infty}  \frac{D(t)}t \frac{D(m, t)}{D(t)}\frac{ D(m, n, t)}{D(m,t)} \\
&=   \gamma \delta(m) \lim_{t\to\infty} \frac{ D(m, n, t)}{D(m,t)} \\
&=   \gamma \delta(m) \P{Y>n-m+1}\\
& = \gamma \delta(m) G(n-m+1).
  \end{split}
\end{equation*}
\end{solution}
\end{exercise}

Equating the downcrossing and upcrossing rates and dividing
by $\gamma$ gives
\begin{equation*}
  f(0) \delta(n+1) = \delta(0) G(n) + \sum_{m=1}^{n} \delta(m) G(n+1-m).
\end{equation*}
Noting that $\pi(n) = \delta(n)$, which follows from~\eqref{eq:39} and
the fact that the $M/G/1$ queue length process has one-step
transitions, we arrive at
\begin{equation}\label{eq:72}
  f(0) \pi(n+1) = \pi(0) G(n) + \sum_{m=1}^{n} \pi(m) G(n+1-m).
\end{equation}

Clearly, we have  again obtained a recursion by which we can compute, iteratively, the
state probabilities.  Setting $\pi(0)=1$ at first, $\pi(1)$ follows
from the above recursion. Then, with $\pi(0)=1$ and $\pi(1)$ we find
$\pi(2)$, and so on, up to some limit $N$. Then, set
$G=\sum_{n=0}^N \pi(n)$ as the normalization constant. 



\begin{exercise}
  Clearly, the $M/M/1$ queue is a special case of the $M/G/1$
  queue. Apply Eq.~\eqref{eq:72} to the $M/M/1$ queue to obtain $p(n)=(1-\rho)\rho^n$. 
  \begin{hint}
 Define shorthands
    such as $\alpha=\lambda/(\lambda+\mu)$, so that
    $1-\alpha = \mu/(\lambda+\mu)$, and
    $\alpha/(1-\alpha) = \lambda /\mu = \rho$.  Then, with Eq.~\ref{eq:41}, $f(n) = \alpha^n(1-\alpha)$ and $G(n) = \alpha^{n+1}$.  (These results do not `come for
    `free'. Of course, it's just algebra, but please try to derive this
    yourself. Its good to hone your computational skills.)
  \end{hint}
    \begin{solution}
      Take $n=0$. Then $f(0) \pi(1) = (1-\alpha)\pi(1)$, and
      $\pi(0)G(0) = \pi(0)\alpha$. Thus,
      $\pi(1) = \pi(0)\alpha/(1-\alpha) = \rho \pi(0)$. 

For $n=1$,
\begin{equation*}
  \begin{split}
  (1-\alpha)  \pi(2) 
&= \pi(0)G(1) + \pi(1)G(1) = \pi(0)G(1)(1+\rho) \\
&= \pi(0)\alpha^2(1+\rho) = \pi(0)\alpha \alpha (1+\rho) = \pi(0)\alpha \rho.
  \end{split}
\end{equation*}
Dividing by $1-\alpha$, we get
\begin{equation*}
  \pi(2) = \pi(0)\rho^2
\end{equation*}

Finally, now that we suspect that $\pi(n) = \rho^n \pi(0)$, let's fill
it in, and see whether it checks. We divide both sides by $\pi(0)$ so
that we are left with checking that
\begin{equation*}
  \begin{split}
    (1-\alpha)\rho^{n+1} 
&= \alpha^{n+1} + \sum_{m=1}^n \rho^m \alpha^{n-m+2}  \\
&= \alpha^{n+1} + \alpha^{n+2}\sum_{m=1}^n (\rho/\alpha)^m  \\
&= \alpha^{n+1} + \alpha^{n+1}\rho \sum_{m=0}^{n-1} (\rho/\alpha)^m \\
&= \alpha^{n+1} + \alpha^{n+1}\rho \sum_{m=0}^{n-1} (\rho/\alpha)^m\\
&= \alpha^{n+1} + \alpha^{n+1}\rho \frac{1-(\rho/\alpha)^n}{1-\rho/\alpha)}\\
&= \alpha^{n+1} - \alpha^{n+1}(1-(\rho/\alpha)^n), \quad\text{as } \rho/\alpha = 1+\rho,\\
&= \alpha^{n+1}(\rho/\alpha)^n = \alpha \rho^n.\\
  \end{split}
\end{equation*}
Since $\rho=\alpha/(1-\alpha)$ we see that the left and right hand sides are the same. 

Thus we get that $\pi(n) = \delta(n) = (1-\rho) \pi(0)$; a result we
obtained earlier for the $M/M/1$ queue.
\end{solution}
\end{exercise}


\begin{exercise}
  Can you design a suitable numerical method to evaluate   Eq.~\eqref{eq:10} for more 
  general distribution functions $F$? (This topic is not strictly necessary for this course, but I hope that you study it nonetheless.)
\begin{hint}
Discretize time to  a grid of points, and   approximate the integral by a summation over the grid.
\end{hint}
  \begin{solution}
  A simple numerical method is as follows. Make a grid of
  size $\d x$, for some small number $\d x$, e.g. $\d x=1/100$, and write
  $f_i = \P{S\in(i\d x, (i+1)\d x]} = F((i+1)\d x) - F(i\d x)$. Then 
  \begin{equation*}
    \begin{split}
  \P{Y_k = j} 
&= \int_0^\infty e^{-\lambda s}\frac{(\lambda x)^j}{j!} \d F(x) 
\approx \sum_{i=1}^\infty e^{-\lambda i \d x}\frac{(\lambda i\d x)^j}{j!} f_i \d x
    \end{split}
\end{equation*}

Lets try a numerical experiment. 


\begin{lstlisting}
import numpy as np

labda = 3
mu = 4
j = 5
dx = 1 / 100


def F(x):
    return 1 - np.exp(-mu * x)


def f(x):
    return F(x + dx) - F(x)


def term(i):
    res = np.exp(-labda * i * dx)
    res *= (labda * i * dx)**j / np.math.factorial(j)
    res *= f(i*dx) * dx
    return res
\end{lstlisting}


\begin{lstlisting}
print(sum(term(i) for i in range(50)))
\end{lstlisting}\begin{lstlisting}
1.11588350072e-05
\end{lstlisting}
\begin{lstlisting}
print(sum(term(i) for i in range(500)))
\end{lstlisting}\begin{lstlisting}
8.09880771006e-05
\end{lstlisting}
\begin{lstlisting}
print(sum(term(i) for i in range(5000)))
\end{lstlisting}\begin{lstlisting}
8.09880771273e-05
\end{lstlisting}

Since I don't know when to stop the integral I just try a few values;
of course stopping the integration at $x=50$ is too small, since
$50\d x=50/100=1/2$, but I include it for illustrative
purposes. Stopping at $500$ seems ok, since the results of the last
two integrals are nearly the same. This also suggestst that
$\d x=1/100$ is sufficiently small. In general, however, one must take
care and try various values for $\d x$ and the integration limits.


For more complicated situations is it best to use a
numerical library to compute the above integral. These methods have been
designed to produce good and reliable results, and, typically, it is
very hard to improve these methods. Thus, let's try a real number
cruncher.


\begin{lstlisting}
from scipy.integrate import quad

def g(x):
    return np.exp(-labda*x) * (labda*x)**j/np.math.factorial(j) * f(x)

print(quad(g, 0, np.inf))
\end{lstlisting}\begin{lstlisting}
(8.098807712760667e-05, 3.4086429822163874e-11)
\end{lstlisting}

This is the same as our earlier answer.
\end{solution}
\end{exercise}


\begin{comment}
\begin{exercise}[use=false]
  To analyze the distribution of $L$ of the $M/G/1$ we concentrated on
  the departure epochs. Why will this not help to find the
  distribution of $L$ for the $M/G/c$ queue?
  \begin{solution}
    \TBD.
  \end{solution}
\end{exercise}
\end{comment}


\Closesolutionfile{hint}
\Closesolutionfile{ans}
\subsection*{Hints}
\input{hint}
\subsection*{Solutions}
\input{ans}
\clearpage



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "book"
%%% End:
