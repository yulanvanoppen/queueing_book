\section{Open Single-Class Product-Form Networks}
\label{sec:jackson-networks}


\subsection*{Theory and Exercises}

\Opensolutionfile{hint}
\Opensolutionfile{ans}

The remark above Zijm.Eq.2.11 is not entirely correct. Remove the
sentence: `These visit ratios satisfy \ldots up to a multiplicative
constant'.


I don't like the derivation of Zijm.Eq.2.20. The appearance of the
visit ratios $\lambda_i/\gamma$ seems to come out of thin air. The
argument should be like this. Consider the entire queueing network as
one `box' in which jobs enter at rate $\gamma=\sum_{i=1}^M
\gamma_i$.
Assuming that there is sufficient capacity at each station, i.e.,
$\lambda_i < c_i \mu_i$ at each station $i$, the output rate of the `box' must also be $\gamma$. Thus, by applying Little's law to the `box', we have that 
\begin{equation*}
  \E L = \gamma \E W. 
\end{equation*}
It is also evident that the average total number of jobs must be equal
to the sum of the average number of  jobs at each station: 
\begin{equation*}
  \E L = \sum_{i=1}^M \E{L_i}.
\end{equation*}
Applying Little's law to each station separately we get that
$\E{L_i} = \lambda_i\E{W_i}$. Filling this into the above,
\begin{equation*}
\E W = \frac{\E L}{\gamma}  = \sum_{i=1}^M \frac{\E{L_i}}\gamma = \sum_{i=1}^M \frac{\lambda_i \E{ W_i}}\gamma, 
\end{equation*}
where we recognize the visit ratios.


\begin{exercise}[\faFlask\/ Linear algebra refresher] Can you find an example to
  show for two matrices $A$ and $B$ that $AB\neq BA$, hence
  $x A \neq A x$.
  \begin{hint}
    Let 
    \begin{equation*}
A =
    \begin{pmatrix}
      1 & 1 \\ 
0&1
    \end{pmatrix},
\quad   B=
    \begin{pmatrix}
      1 & 0 \\ 
1&1
    \end{pmatrix}.
    \end{equation*}
  \end{hint}

  \begin{solution}
    \begin{equation*}
      AB =  
    \begin{pmatrix}
      2 & 1 \\ 
1&1
    \end{pmatrix} 
\neq
    \begin{pmatrix}
      1 & 1 \\ 
1&2
    \end{pmatrix} 
= BA.
    \end{equation*}

Take $x=(1,1)$, then $x A=(1,2)$. Now, taking $x=
\begin{pmatrix}
  1 \\
1
\end{pmatrix}
$, we get $Ax = 
\begin{pmatrix}
  2 \\
1
\end{pmatrix}.  $
Recall, horizontal vectors are not vertical vectors. The horizontal
ones are to the left of a matrix, and the vertical ones to the right.
  \end{solution}
\end{exercise}

\begin{exercise}[\faCalculator\/ Linear algebra refresher 2] Suppose the matrix $A$
  has an eigenvalue $0$. What is the geometric meaning of this fact? 
  \begin{solution}
    Many students think that a matrix is just a bunch of numbers ordered in a grid.
    This is, in my opinion, the most unproductive way to think about matrices.
    A much more useful way is to see a matrix as an \emph{operator}.
    For instance, take $A$ to be a $3\times3$ matrix.
    Then it can be seen as a \emph{mapping} from $\R^3$ to $\R^3$; it takes a vector $x\in\R^3$ and changes $x$ into a new vector $Ax\in \R^3$.
    Thus, a square matrix $A$ typically changes the length and direction of a vector $x$.

    The next example is meant to illustrate what happens when a matrix has an eigenvalue 0. Consider the simple example with
    \begin{equation*}
A =
    \begin{pmatrix}
      1 & 0 & 0 \\ 
      0 & 1 & 0 \\ 
      0 & 0 & 0 \\ 
    \end{pmatrix}.
  \end{equation*}
Clearly, $A$ has an eigenvalue $0$. Now take $v=(x,y,z)'$, so that
    \begin{equation*}
A v = A
 \begin{pmatrix}
x\\
y\\
z
    \end{pmatrix}
= \begin{pmatrix}
x\\
y\\
0
    \end{pmatrix}.
  \end{equation*}
  We see that $A$ removes any information about the $z$-direction from the vector $v$.
  (It projects $v$ on the $x-y$ plane, and throws away the $z$ component of $v$.)
  But then, for a given vector $w=(x,y,0)$ in the $x-y$ plane, it is impossible to use $A$ to retrieve the original vector $v=(x,y,z)$.
  Thus, $A$ cannot have an inverse on all of $\R^3$.

  So, hopefully, with this example, you can memorize that for any matrix $A$ to have an inverse, it is essential that it has no zero eigenvalues.
  When the \emph{operator} $A$ (don't think of a matrix as a set of numbers) throws away part of the dimension of the space on which it operates (i.e., it has one or more eigenvalue(s) $0$), it is impossible to retrieve the part of the space it throws away.
  Hence, its inverse cannot be used to get this part of the space back.

\end{solution}


\end{exercise}


\begin{exercise}[\faFlask]
Zijm.Ex.2.2.1
\begin{solution}
Because jobs cannot leave faster than they arrive.
\end{solution}
\end{exercise}

\begin{exercise}[\faFlask]
Zijm.Ex.2.2.2
\begin{solution}
  Observe from Exercise~\ref{ex:dep} that the inter-departure times of the $M/M/1$ queue are also independent and identically exponentially distributed with rate $\lambda$.
  Since the arrival process at the second station is the departure process of the first station, it must be that the arrival process at the second station is also Poisson with rate $\lambda$.
  Interestingly, from the perspective of the second station it is as if there is no first station.
\end{solution}
\end{exercise}



\begin{exercise}[\faFlask]
Zijm.Ex.2.2.3 
\begin{solution}
  The question is not well specified. We know from Burke's law, see Exercise~\ref{ex:burke}, that
  the arrival \emph{process} at the second station is Poisson.  If,
  however, we know that station 1 is empty, then it is unlikely that a
  job will arrive at station 2 in the very near future.

  Note that only the steady-state distributions of the queue lengths
  are independent. Once you have information about the state of one of
  the queues, then certainly this is not in `steady-state'.
\end{solution}
\end{exercise}

\begin{exercise}[\faFlask]
Zijm.Ex.2.2.4
\begin{solution}
  Simple algebra.  (I am not going to write it out here. If you are
  willing to provide me the answer in \LaTeX\/ I'll include it.)
\end{solution}
\end{exercise}

\begin{exercise}[\faCalculator]
  Zijm.Ex.2.2.5. The problem is not entirely correctly formulated. It
  should be, if for at least one $i$, $\sum_{j=1}^M P_{i j} <1$ \ldots
\begin{solution}
Linear algebra is quite useful here!

Observe that $P_{i j}$ is the probability that a job, after completing its service at node $i$, moves to node $j$.
Then $\sum_{j=1}^M P_{i j}$ is the probability that a job moves from node $i$ to another node in the network, i.e., stays in the network, while $P_{i0}$ is the probability that a job leaving node $i$ departs the network, in other words, the job is finished.
When $\sum_{j=1}^M P_{i j} < 1$, then more jobs enter node $i$ from the network than that node $i$ sends `back' into the network.
Conceptually, node $i$ `leaks jobs'.

Now, consider some node $k$ such that $P_{ki} > 0$, then the probability that a job that starts at node $k$, moves to node $i$ and then leaves the network is equal to $P_{ki}P_{i0}$.
Thus, since $P_{ki}>0$ and $P_{i0}>0$, the probability that a job leaves the network from node $k$ in two steps is positive.
More specifically, $P^2_{k0} = \sum_{j=0}^M P_{kj}P_{j0} \geq P_{ki}P_{i0} > 0$.

The irreducibility assumption implies that in at most $M$ steps it is possible to reach, with positive probability, any node from any other node in the network.
Thus, for any node $j$ to any other node $k$ there is a sequence of nodes $j_1, j_2, \ldots , j_{M-1}$ such that $P^{M}_{jk} \geq P_{j j_1}P_{j_1 j_2}\cdots P_{j_{M-1}k} > 0$.


Thus, if there is a node $i$ such that $P_{i0}>0$, then it is possible from any node that sends jobs to node $i$ directly to leave the network in two steps.
Likewise, when node $i$ can be reached from node $k$ in $n$ steps, say, then $P^{n+1}_{k0} \geq P^n_{ki}P_{i0} > 0$, i.e., in at most $n+1$ steps it is possible to leave the network from such node $k$.
This implies, in particular, that for all nodes $k=1,2,\ldots, M$, i.e., all nodes in the network, $P^{M+1}_{k0} >0$.
For this reason we consider $P^{M+1}$ in the hint.


As a final remark for students with knowledge of Markov chains, observe that the routing matrix $P$ does not correspond to the transition matrix of a recurrent Markov chain.
Since for at least one row $i$, $\sum_{j=1}^N P_{i j}<1$, the matrix $P$ is sub-stochastic.
Hence, a Markov chain induced by $P$ cannot be irreducible, because for this to happen, the chain must stay in some absorbing set with probability 1.
\end{solution}
\end{exercise}

\begin{exercise}[\faCalculator]
\label{ex:20}
Zijm.Ex.2.2.6
\begin{solution}
  Since $M$ is finite, and $k\leq M$, the set of numbers $P^{M+1}_{k0}$ is finite.
  This, together with the fact that $P^{M+1}_{k0}>0$ for all $k$, implies that there is some number $\epsilon>0$ such that $P^{M+1}_{k0}>\epsilon$.
  Hence, for all entries $k=1, 2, \ldots, M$, we have that $P^{M+1}_{kj} < 1- \epsilon$.
  This, in turn, implies that $P^{2(M+1)}_{kj} < (1- \epsilon)^2$, and so on, so that for any $n$, $P^{n(M+1)}_{kj} < (1- \epsilon)^n$.
  This implies, in more general terms, that the entries of $P^n$ decrease geometrically fast to $0$.

  It is well known that for any bounded sequence $x_i$ and $0\leq \alpha < 1$, $ \sum_{i=0}^\infty x_i \alpha^i < \infty$.
  By applying this insight to the entries of $P^n$ it follows that $\sum_{n=0}^\infty P^n_{jk} < \infty$.

Finally, applying $\lambda = \gamma + \lambda P$ recursively, we get
\begin{equation*}
  \lambda = \gamma + \lambda P = \gamma + (\gamma + \lambda P)P = \gamma (1+P) + \lambda P^2 = \gamma(1+P+P^2) + \lambda P^3 \to \gamma \sum_{n=0}^\infty P^n.
\end{equation*}
By the above reasoning this last sum is well defined, and finite.
(For math aficionados:  the above argument is not necessarily valid for matrices $P$ that are infinite, since then $\inf\{P^{M}_{ik}\}$ need not be strictly positive.)

Another interesting way to see all this is by making the simplifying assumption that $P$ is a diagonalizable matrix.
(The argument can be generalized to include matrices reduced to Jordan normal form, but this gives optimal clutter, but does not change the line of reasoning in any fundamental way.)
In that case, there exists an invertible matrix $V$ with the (left) eigenvectors of $P$ as its rows and a diagonal matrix $\Lambda$ with the eigenvalues on its diagonal such that
\begin{equation*}
  V P = \Lambda V.
\end{equation*}
Hence, premultiplying with $V^{-1}$, 
\begin{equation*}
  P = V^{-1}\Lambda V.
\end{equation*}
But then
\begin{equation*}
P^2 = V^{-1}\Lambda V \cdot V^{-1}\Lambda V= V^{-1}\Lambda^2 V,
\end{equation*}
and in general $P^n = V^{-1}\Lambda^n V$.
If each eigenvalue $\lambda_i$ is such that its modulus $|\lambda_i| < 1$, then $\Lambda^n \to 0$ geometrically fast, hence $P^n\to 0$ geometrically fast, hence the sequence of partial sums $\sum_{n=0}^N P^n$ converges to a matrix with finite elements as $N\to\infty$.

So, we are left with proving that the eigenvalues of $P$ must have modulus less than $1$.
This fact follows from Gerschgorin's disk theorem, which I include for the interested student.
Define the disk $B(a,r)=\{z\in \mathbb{C} | |z-a|\leq r\}$, i.e., the set of complex numbers such that the distance to the center $a\in \mathbb{C} $ is less than or equal to the radius $r$.
With this, the Gerschgorin disks of a matrix are defined as $B(a_{ii}, \sum_{j\neq i} |a_{i j}|)$, i.e., disks with center at the diagonal elements $a_{ii}$ of $A$ and radius equal to the sum of the (modulus of the) elements of $A$ on the $i$th row except $a_{ii}$.
Then Gerschgorin's theorem says that all eigenvalues of $A$ lie in the union of these disks, i.e., all eigenvalues $\lambda_i \in \bigcup_i B({a_{ii}, \sum_{j\neq i}|a_{i j}})$.

Assume for notational simplicity that for each row $i$ of $P$ we have
that $\sum_{j} a_{i j}<1$. (Otherwise apply the argument to $P^{M+1}$.)
Then this implies for all $i$ that
\begin{equation*}
  a_{ii} + \sum_{j\neq i} a_{i j} < 1. 
\end{equation*}
Since all elements of $P$ are non-negative, this also implies that
\begin{equation*}
-1 <   a_{ii} - \sum_{j\neq i} a_{i j} \leq  a_{ii} + \sum_{j\neq i} a_{i j} < 1. 
\end{equation*}
With this and using that $a_{ii}$ is a real number (so that it lies on the real number axis) it follows that all elements in the disk $B(a_{ii}, \sum_{j\neq i} a_{i j})$ have modulus smaller than 1.
As this applies to any row $i$, all disks lie strictly within the complex unit circle.
But then, by Gerschgorin's theorem, all eigenvalues of $P$ also lie strictly in the unit circle, hence all eigenvalues have modulus smaller than 1.
\end{solution}
\end{exercise}

\begin{exercise}[\faCalculator]
\label{ex:23}
  Show that Zijm.Eq.2.13 and 2.14 can be written as
  \begin{equation*}
    f_i(n_i) = \frac{1}{G(i)}\frac{1}{\Pi_{k=1}^{n_i} \min\{k, c_i\}}\left( \frac{\lambda_i}{\mu_i}\right)^{n_i}.
  \end{equation*}
  \begin{solution}
    Take $n_i<c_i$. Then
    $\Pi_{k=1}^{n_i} \min\{k, c_i\} = \Pi_{k=1}^{n_i} k = n_i!$, and
    $(c_i\rho_i)^{n_i} = (\lambda_i/\mu_i)^{n_i}.$ If $n_i\geq c_i$,
    then $\Pi_{k=1}^{n_i} \min\{k, c_i\} = c_i! c_i^{n_i-c_i}$, and
    $(c_i\rho_i)^{n_i} = (\lambda_i/\mu_i)^{n_i} c_i^{n_i}$.
  \end{solution}
\end{exercise}


\begin{exercise}[\faCalculator]
Zijm.Ex.2.2.7
\begin{solution}
  \begin{equation*}
    P = 
    \begin{pmatrix}
      \alpha & 1- \alpha \\
      \beta_1 & \beta_2
    \end{pmatrix}.
  \end{equation*}

  \begin{equation*}
    (\lambda_1, \lambda_2) = (\gamma, 0) + (\lambda_1, \lambda_2) P.
  \end{equation*}
Solving first for $\lambda_2$ leads to $\lambda_2 = (1-\alpha) \lambda_1 + \beta_2 \lambda_2$, so that  
\begin{equation*}
  \lambda_2 = \frac{1-\alpha}{1-\beta_2} \lambda_1. 
\end{equation*}
Next, using this and that $\lambda_1 = \alpha \lambda_1 + \beta_1 \lambda_2 + \gamma$ gives with a bit of algebra
\begin{equation*}
  \begin{split}
\gamma 
&=  \lambda_1(1-\alpha) - \beta_1\lambda_2 \\
&=  \lambda_1\left(1-\alpha - \beta_1\frac{1-\alpha}{1-\beta_2}\right) \\
&=  \lambda_1(1-\alpha)\left(1 - \frac{\beta_1 }{1-\beta_2}\right) \\
&=  \lambda_1(1-\alpha)\frac{1-\beta_1-\beta_2 }{1-\beta_2}.
  \end{split}
\end{equation*}
Hence, 
\begin{equation*}
  \lambda_1 = \frac\gamma{1-\alpha}\frac{1-\beta_2}{1-\beta_1-\beta_2}. 
\end{equation*}
Thus, 
\begin{equation*}
  \lambda_2 = \frac{1-\alpha}{1-\beta_2} \lambda_1 = \frac\gamma{1-\beta_1-\beta_2}. 
\end{equation*}


We want of course that $\lambda_1 < \mu_1$ and $\lambda_2 < \mu_2$.
With the above expressions this leads to conditions on $\alpha$, $\beta_1$ and $\beta_2$.
Note that we have three parameters, and two equations; there is not a single condition from which the stability can be guaranteed.

If $\alpha\uparrow 1$, the arrival rate at node $1$ explodes.
If $\beta_1=0$ no jobs are sent from node 2 to node 1.
\end{solution}
\end{exercise}

\begin{exercise}[\faFlask]
Zijm.Ex.2.2.8
\begin{solution}
  Yes, the network remains a Jackson network. By Burke's law, see Exercise~\ref{ex:burke}, the
  departure process of each node is Poisson. In one of the earlier
  questions we derived that splitting (also known as thinning) and
  merging Poisson streams again lead to Poisson streams. The
  departures from node $j$ to node $k$ forms a thinned Poisson
  stream. The external arrivals plus internal arrivals are merged into
  one Poisson stream, hence the arrivals at a station also form a Poisson stream.

  Observe that the exponentiality of the service times and external
  inter-arrival times and Burke's law are essential for the argument.
\end{solution}
\end{exercise}




\Closesolutionfile{hint}
\Closesolutionfile{ans}

\opt{solutionfiles}{
\subsection*{Hints}
\input{hint}
\subsection*{Solutions}
\input{ans}
}
%\clearpage



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../queueing_book"
%%% End:
