\section{Poisson Distribution}
\label{sec:poisson-distribution}

\subsection*{Theory and Exercises}


\Opensolutionfile{hint}
\Opensolutionfile{ans}

In this section we provide motivation for the use of the Poisson process as an arrival process of customers or jobs at a shop, service station, or machine, to receive service.  In the exercises we derive numerous  properties of  this exceedingly important distribution; in the rest of the book we will use these results time and again.

Consider a stream of customers that enter a shop over time.
Let us write $N(t)$ for the number of customers that entered during the time interval $[0,t]$ and, with this, $N(s, t] = N(t)-N(s)$.
Clearly, as we do not know in advance how many customers will enter, we model the set $\{N(t), t\geq 0\}$ as a family of  random variables.

Our first assumption is that the rate at which customers enter stays constant over time. Then it is reasonable to assume that
the expected number of arrivals is proportional to the  length of
the interval. Hence, it is reasonable to assume that there exists some
constant~$\lambda$ such that
\begin{equation}
  \label{eq:6}
 \E{N(s,t]} = \lambda (t-s).
\end{equation}
The constant $\lambda$ is called the \recall{arrival rate} of the arrival process.


The second assumption is that the process $N_\lambda = \{N(t), t\geq 0\}$ has \recall{stationary and independent increments}.
Stationarity means that the distributions of the number of arrivals are the same for all intervals of equal length, that is, 
$N(s,t]$ has the same distribution as $N(u, v]$ if $t-s = v-u$.
Independence means, roughly speaking, that knowing that $N(s,t]= n$, does not help to make any predictions about the value of $N(u, v]$ if the intervals $(s,t]$ and $(u, v]$ do not overlap.



To find the distribution of $N(t)$ for some given $t$, let us split the interval $[0,t]$ into $n$ sub-intervals, all of equal length, and ask: `What is the probability that a customer arrives in some given sub-interval?'
By our second assumption, the arrival rate is constant over time.
Therefore, the probability~$p$ of an arrival in each interval should be constant.
Moreover, if the time intervals are very small, we can safely neglect the probability that two or more customers arrive in one interval.

As a consequence, then, we can model the occurrence of an arrival in
some period $i$ as a Bernoulli distributed random variable $B_i$ such
that $p=\P{B_i =1}$ and $\P{B_i=0} = 1-\P{B_i = 1}$, and we assume that
$\{B_i\}$ are independent. The total number of arrivals $N_n(t)$ that
occur in $n$ intervals is then binomially distributed
\begin{equation}\label{eq:bin}
  \P{N_n(t) = k} = {n \choose k} p^k (1-p)^{n-k}.
\end{equation}

\begin{exercise}[\faFlask]
Show that $\E{N_n(t)} = \sum_{i=1}^n \E{B_i} = n p$. Conclude that we need to choose $p = \lambda t/n$ if we want that $\E{N_n(t)} = \E{N(t)}$.
\begin{hint}
Use that $\E{X+Y} = \E X + \E Y$. 
\end{hint}
\begin{solution}
  \begin{equation*}
    \E{N_n(t)} = \E{\sum_{i=1}^n {B_i}} = \sum_{i=1}^n \E{B_i} = n \E{B_i} = n p.
  \end{equation*}

 By~\eqref{eq:6} $\E{N(t)} = \lambda t$.  We want the expectations of $N_n(t)$ and $N(t)$ to be the same,  that is, $n p = \lambda t$. 
\end{solution}
\end{exercise}

\begin{exercise}[\faFlask]
What is the difference between $N_n(t)$ and $N(t)$?
\begin{solution}
  $N_n(t)$ is a binomially distributed random variable with parameters
  $n$ and $p$. The maximum value of $N_n(t)$ is $n$. The random
  variable $N(t)$ models the number of arrivals that can occur during
  $[0,t]$. As such it is not necessarily bounded by $n$. Thus, $N_n(t)$ and $N(t)$ cannot represent the same random variable. 
\end{solution}
\end{exercise}




\begin{exercise}[\faCalculator]\label{ex:31}
  Show that the binomial distribution in~\eqref{eq:bin} converges to the \recall{Poisson distribution}, i.e., 
%\begin{equation}\label{eq:pois}
 \begin{equation*}\label{eq:52}
   \lim_{n\to\infty} {n \choose k} \left(\frac{\lambda t}{n}\right)^k \left(1-\frac{\lambda t}n\right)^{n-k} = e^{-\lambda t} \frac{(\lambda t)^k}{k!},
 \end{equation*}
% e^{-\lambda t} \frac{(\lambda t)^k}{k!}
% \end{equation}
if $n\to\infty$, $p\to0$ such
  that $n p=\lambda t$. 
  \begin{hint}
First find $p$, $n$, $\lambda$ and $t$ such that the rate
    at which an event occurs in both processes are the same. Then consider
    the binomial distribution and use the standard limit
    $(1-x/n)^n \to e^{-x}$ as $n\to \infty$. 
  \end{hint}
  \begin{solution}
% Substituting this in the expression for $\P{N_n(t)=k}$ gives
% \begin{equation*}
%   \P{N_n(t) = k} = {n \choose k} \left(\frac{\lambda t}{n}\right)^k \left(1-\frac{\lambda t}n\right)^{n-k}.
% \end{equation*}
% To see that 
% \begin{equation*}\label{eq:52}
%   \lim_{n\to\infty} {n \choose k} \left(\frac{\lambda t}{n}\right)^k \left(1-\frac{\lambda t}n\right)^{n-k} = e^{-\lambda t} \frac{(\lambda t)^k}{k!},
% \end{equation*}
% use that
    \begin{align*}
      {n \choose k} \left(\frac{\lambda t}{n}\right)^k \left(1-\frac{\lambda t}n\right)^{n-k} 
&= \frac{n!}{k!(n-k)!} \left(\frac{\lambda t}{n}\frac{n}{n-\lambda t}\right)^k \left(1-\frac{\lambda t}n\right)^{n} \\
&= \frac{(\lambda t)^k}{k!} \left(\frac n{n-\lambda t} \right)^k  \frac{n!}{n^k(n-k)!}\left(1-\frac{\lambda t}n\right)^{n}\\
&= \frac{(\lambda t)^k}{k!} \left(\frac n{n-\lambda t} \right)^k \frac{n}{n}\frac{n-1}{n}\cdots\frac{n-k+1}{n} \left(1-\frac{\lambda t}n\right)^{n}.
\end{align*}
Observe now that, as $\lambda t$ is finite, $n/(n-\lambda t)\to 1$ as
$n\to \infty$. Also for any finite $k$, $(n-k)/n\to1$. Finally,  use~\eqref{eq:65} to see that
$\left(1-\frac{\lambda t}n\right)^{n} \to e^{-\lambda t}$.
% The rest
% is easy, so that, as $n\to\infty$,  the above converges to 
% \begin{equation*}
% \frac{(\lambda t)^k}{k!} e^{-\lambda t}.
% \end{equation*}

  \end{solution}
\end{exercise}

We say that the random variable $N(t)$ is \emph{Poisson distributed} when 
\begin{equation}\label{eq:pois}
  \P{N(t) = k} = 
e^{-\lambda t} \frac{(\lambda t)^k}{k!}, 
\end{equation}
and then we write $N(t)\sim P(\lambda t)$.
Moreover, we call the process $N_\lambda=\{N(t)\}$ a \recall{Poisson process} with rate $\lambda$ when $N_\lambda$ is stationary, has independent increments, and its elements $N(t)\sim P(\lambda t)$ for all $t$.
Observe that the process $N_\lambda$ is a much more complicated object than a Poisson distributed random variable.
The process is an uncountable set of random variables indexed by $t\in \R^+$, not just \emph{one} random variable.


In the remainder of this section we derive a number of  properties of the Poisson process that we will use time and again.

\begin{exercise}[\faCalculator]\label{ex:2}
  Show that if $N(t)\sim P(\lambda t)$, the expected number of arrivals during $[0,t]$ is
  \begin{equation*}
  \E{N(t)} = \lambda t.
  \end{equation*}
  \begin{hint}
Use~\eqref{eq:66}. Note  that the term with $n=0$ does not contribute in the following summation
\begin{equation*}
\sum_{n=0}^\infty n \frac{\lambda^n}{n!} = \sum_{n=1}^\infty n \frac{\lambda^n}{n!} = \sum_{n=1}^\infty \frac{\lambda^n}{(n-1)!} = \lambda \sum_{n=0}^\infty \frac{\lambda^n}{n!} = \lambda e^{\lambda},
\end{equation*}
where we apply a change of notation in the second to last step.
  \end{hint}
  \begin{solution} 
    When a random variable~$N$ is Poisson distributed with parameter
    $\lambda t$,
    \begin{align*}
      \E N 
&= \sum_{n=0}^\infty n e^{-\lambda t}\frac{(\lambda t)^n}{n!}  \\
&= \sum_{n=1}^\infty n e^{-\lambda t}\frac{(\lambda t)^n}{n!} \\ 
&= e^{-\lambda t} \lambda t \sum_{n=1}^\infty \frac{(\lambda t)^{n-1}}{(n-1)!} \\
&= e^{-\lambda t} \lambda t \sum_{n=0}^\infty \frac{(\lambda t)^{n}}{n!} \\
&= e^{-\lambda t} \lambda t e^{\lambda t} \\
&= \lambda t.
    \end{align*}
\end{solution}
\end{exercise}


\begin{exercise}[\faCalculator]
  Show that if $N(t)\sim P(\lambda t)$, the variance of the number of arrivals during $[0,t]$ is
  \begin{equation*}
\V{N(t)} = \lambda t.
  \end{equation*}
  \begin{hint} Use~\eqref{eq:68}. Compute $\E{N^2}$ and use Exercise~\ref{ex:2}.
  \end{hint}
  \begin{solution} 

    \begin{align*}
      \E{N^2}
&= \sum_{n=0}^\infty n^2 e^{-\lambda t}\frac{(\lambda t)^n}{n!}  \\
&= e^{-\lambda t} \sum_{n=1}^\infty n \frac{(\lambda t)^n}{(n-1)!}  \\
&= e^{-\lambda t} \sum_{n=0}^\infty (n+1) \frac{(\lambda t)^{n+1}}{n!}  \\
&= e^{-\lambda t} \lambda t \sum_{n=0}^\infty n \frac{(\lambda t)^{n}}{n!}  +e^{-\lambda t}\lambda t \sum_{n=0}^\infty\frac{(\lambda t)^{n}}{n!}  \\
&\quad\text{now use the hint to simplify the first summation} \\
&= (\lambda t)^2  + \lambda t.
\end{align*}
Hence, $\V N = \E{N^2} - (\E N)^2 = (\lambda t)^2 + \lambda t - (\lambda t)^2 = \lambda t$.
\end{solution}
\end{exercise}

With moment-generating functions we can compute the mean and variance of $N(t)$ with less effort. 
\begin{exercise}[\faCalculator]
Show that the moment-generating function of the random variable~$N(t)\sim P(\lambda t)$ is
\begin{equation*}
M_{N(t)}(s) 
%=  \E{e^{sN(t)}}  
= \exp{(\lambda t(e^s-1))}.
\end{equation*}
\begin{hint}
Use~\eqref{eq:66} with $f(x) = e^{sx}$.  \end{hint}
\begin{solution}
Since $N(t)$ is Poisson distributed with parameter $\lambda t$, 
\begin{align*}
M_{N(t)}(s)
&=  \E{e^{s N(t)}} \\
&= \sum_{k=0}^\infty e^{s k} \P{N(t)=k} \\
%&\quad\text{where we use that $\E{f(N(t))}=\sum_{n=0}^\infty f(n) \P{N(t)=n}$}\\
&= \sum_{k=0}^\infty e^{s k} \frac{(\lambda t)^k}{k!} e^{-\lambda t}  \\
&= e^{-\lambda t} \sum_{k=0}^\infty  \frac{(e^s \lambda t)^k}{k!}  \\
&= \exp(-\lambda t + e^s \lambda t) =\exp(\lambda t(e^s - 1)).
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}[\faCalculator]
Use  the moment-generating function of $N(t)\sim P(\lambda t)$ to compute $\E{N(t)}$ and $\V{N(t)}$. 
\begin{hint}
Use~\eqref{eq:69} and~\eqref{eq:64}. 
\end{hint}
\begin{solution}
Using the expression for the moment-generating function of the previous exercise,
  \begin{equation*}
    M_{N(t)}'(s) = \lambda t e^s \exp(\lambda t(e^s - 1)).
  \end{equation*}
Hence $\E{N(t)} = M_{N(t)}'(0) = \lambda t $. Next, 
  \begin{equation*}
    M_{N(t)}''(s) = (\lambda t e^s + (\lambda t e^s)^2) \exp(\lambda t(e^s - 1)),
  \end{equation*}
hence $\E{(N(t))^2} = M''(0) = \lambda t + (\lambda t)^2$. And thus, 
\begin{equation*}
\V{N(t)} =\E{(N(t))^2}-(\E{N(t)})^2 = \lambda t + (\lambda t)^2 - (\lambda t)^2 = \lambda t.
\end{equation*}
\end{solution}
\end{exercise}

Define the \recall{square coefficient  of variation} (\recall{SCV}) of a random variable~$X$ as 
\begin{equation}\label{eq:62}
  C^2= \frac{\V X}{(\E X)^2}.
\end{equation}
As  will become clear later, the SCV is a very important concept in
  queueing theory. Memorize it as a measure of \emph{relative
  variability}.

\begin{exercise}[\faFlask]
Show that   the SCV of $N(t)\sim P(\lambda t)$ is equal to $1/(\lambda t)$.  What does this mean for $t$ large?
  \begin{solution}
    \begin{equation*}
SCV = \frac{\V{N(t)}}{(\E{N(t)})^2} = \frac{\lambda t}{(\lambda t)^2} = \frac1{\lambda t}.
    \end{equation*}
The relative variability of the Poisson process goes down as $t\to\infty$.  
  \end{solution}
\end{exercise}


\begin{exercise}[\faFlask]\label{ex:35}
  Show that 
  \begin{equation*}
\P{N(t+h) = n \given N(t) = n} = 1-\lambda h + o(h).
  \end{equation*}
when $N(t) \sim P(\lambda t)$  and $h$ is small. 
    \begin{hint}
Use  the definition of the conditional probability of Exercise~\ref{ex:18} and small $o$ notation, cf. Exercise~\ref{ex:18}. 

Think about the meaning of the formula $\P{N(t+h) = n \given N(t) = n}$.
It is a conditional probability that should be read like this: given that up to time $t$ we have seen $n$ arrivals (i.e., $N(t)=n$), what is the probability that just a little later (at $t+h$) the number of arrivals is still $n$, i.e., $N(t+h)=n$?
Then use the definition of the Poisson distribution to compute this probability.
    \end{hint}
\begin{solution}
Write $N(s, t]$ for the number of arrivals in the interval $(s,t]$. First we make a few simple observations: $N(t+h]= N(t) + N(t, t+h]$, hence
\begin{equation*}
  \begin{split}
  \1{N(t+h)=n, N(t)=n}
&=  \1{N(t) + N(t, t+h] = n, N(t)=n} = \\
&\1{N(t, t+h] = 0, N(t)=n}.
  \end{split}
\end{equation*}
Thus, 
    \begin{align*}
  \P{N(t+h) = n | N(t) = n} 
&=  \frac{\P{N(t+h) = n, N(t) = n}}{P{N(t)=n}} \\
&=  \frac{\P{N(t, t+h] = 0,  N(t) = n}}{\P{N(t)=n}} \\
&=  \frac{\P{N(t, t+h] = 0} \P{N(t) = n}}{\P{N(t)=n}}, \quad \text{(independence)}\\
%& \quad\text{(by independence of } N(t, t+h] \text{ and } N(t))\\
&= \P{N(t, t+h] = 0} \\
&= \P{N(0, h] = 0} \quad\text{(stationarity)} \\
&= e^{-\lambda h} (\lambda h)^0/0! \\
&= e^{-\lambda h} = 1-\lambda h + o(h).
    \end{align*}
\end{solution}
\end{exercise}

\begin{exercise}[\faFlask]
  Show that
  \begin{equation*}
\P{N(t+h) = n+1 \given N(t) = n} = \lambda h + o(h)
  \end{equation*}
when $N(t) \sim P(\lambda t)$ and $h$ is small. 
  \begin{hint} Use  Exercise~\ref{ex:35}.
    \end{hint}
\begin{solution}
  \begin{align*}
  \P{N(t+h) = n +1 | N(t) = n} 
&=  \P{N(t+h) = n +1 , N(t) =n}/\P{N(t) = n}\\
&= \P{N(t, t+h] = 1} = e^{-\lambda h} (\lambda h)^1/1! \\
&= (1-\lambda h + o(h))\lambda h  = \lambda h - \lambda^2 h^2 + o(h) \\
&= \lambda h + o(h). 
  \end{align*}
\end{solution}
\end{exercise}

\begin{exercise}[\faCalculator]
  Show that if $N(t) \sim P(\lambda t)$, we have for small $h$,
  \begin{equation*}
 \P{N(t+h) \geq n+2 \given N(t) = n} = o(h).
  \end{equation*}
    \begin{hint}
 Use that  $\sum_{i=2}^\infty x^i/i! = \sum_{i=0}^\infty x^i/i! - x -1 = e^x -x - 1$.
    \end{hint}
\begin{solution}
  \begin{align*}
  \P{N(t+h) \geq n+2 | N(t) = n} 
&= \P{N(t, t+h] \geq 2} \\
&= e^{-\lambda h} \sum_{i=2}^\infty \frac{(\lambda h)^i}{i!} 
= e^{-\lambda h} \left(\sum_{i=0}^\infty \frac{(\lambda h)^i}{i!} - \lambda h - 1\right)\\
&= e^{-\lambda h}(e^{\lambda h} - 1 - \lambda h) 
= 1 - e^{-\lambda h}(1 + \lambda h) \\
&= 1 - (1-\lambda h + o(h))(1+\lambda h) 
= 1 - (1-\lambda^2 h^2 + o(h)) \\
&= \lambda^2 h^2 + o(h) = o(h).
  \end{align*}

We can also use the results of the previous parts to see that
\begin{align*}
  \P{N(t+h) \geq n+2 | N(t) = n} 
&= \P{N(t, t+h] \geq 2} = 1- \P{N(t, t+h]<2} \\
&= 1 - \P{N(t, t+h]= 0} - \P{N(t, t+h]=1} \\
&= 1 - (1-\lambda h + o(h) ) - (\lambda h + o(h)) \\
&= o(h).
\end{align*}
\end{solution}
\end{exercise}


\begin{exercise}[\faCalculator]
Show that  for a Poisson process $N_\lambda$, 
\begin{equation*}
\P{N(s) =1\given N(t)=1} = \frac s t,
\end{equation*}
if $s\in[0,t]$. Thus, if you know that an arrival occurred during $[0,t]$, the arrival is distributed
uniformly on the interval $[0,t]$. Note that this probability is independent of $\lambda$. 
\begin{hint}
 Observe that 
  \begin{equation*}
%\{N(0,s]+N(s,t]=1\}\cap\{N(0,s]=1\} = \{1+N(s,t]=1\}\cap\{N(0,s]=1\}=\{N(s,t]=0\}\cap\{N(0,s]=1\}.
\1{N(0,s]+N(s,t]=1}\1{N(0,s]=1} = \1{1+N(s,t]=1}\1{N(0,s]=1}=\1{N(s,t]=0}\1{N(0,s]=1}.
  \end{equation*}
Use  independence and~\eqref{eq:74}.
\end{hint}
\begin{solution}
From the hint,
\begin{align*}
  \P{N(0,s] =1\given N(0,t]=1} 
&= \frac{\P{N(0,s] =1, N(0,t]=1}}{\P{N(0,t]=1}} \\
&= \frac{\P{N(0,s] =1, N(s,t]=0}}{\P{N(0,t]=1}} \\
&= \frac{\P{N(0,s] =1}\P{N(s,t]=0}}{\P{N(0,t]=1}} \\
&= \frac{\lambda s e^{-\lambda s} e^{-\lambda (t-s)}}{\lambda t e^{-\lambda t}} = \frac s t.
\end{align*}
\end{solution}
\end{exercise}

 


\recall{Merging} Poisson processes occurs often in practice.
We have two Poisson processes, for instance, the arrival processes $N_\lambda$ of men and $N_\mu$ women at a shop.
In the figure below, each cross represents an arrival, in the upper line it corresponds to a man, in the middle line to a woman and in the lower line to an arrival of a general customer at the shop.
Thus, the shop `sees' the superposition of these two arrival processes.
In fact, this merged process $N_{\lambda+\mu}$ is also a Poisson process whose rate is the sum of the rates of the two original Poisson processes.


  \begin{center}
\begin{tikzpicture}[scale=1]
%\draw[[-{Triangle[open]},dotted] (0,10)--(8.5,10);

\draw[->] (0,2)--(10,2);
\node[left] at (0,2) {$N_\lambda(t)$};
\draw[->] (0,1)--(10,1);
\node[left] at (0,1) {$N_\mu(t)$};
\draw[->] (0,0)--(10,0);
\node[left] at (0,0) {$N_{\lambda+\mu}(t)$};

\draw[{Rays[]}-{Rays[]},dotted] (1,2.06)--(1,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (1.5,1.06)--(1.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (3.2,2.06)--(3.2,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (3.5,1.06)--(3.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (4.5,1.06)--(4.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (5,1.06)--(5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (6.1,1.06)--(6.1,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (7.1,2.06)--(7.1,-0.06);
\end{tikzpicture}
\end{center}



\begin{exercise} [\faCalculator]
If the Poisson arrival processes $N_\lambda$ and $N_\mu$  are independent, show with a conditioning argument that
$N_\lambda + N_\mu$ is  a Poisson process with rate $(\lambda + \mu)t$. 
  \begin{hint}
    Use sets $\{N_\lambda(t) = i\}$ to decompose $\{N_\lambda(t) + N_\mu(t) = n\}$. With this observe that
      \begin{equation*}
        \1{N_\lambda(t) + N_\mu(t) = n} = 
        \sum_{i=0}^n \1{N_\lambda(t)=i, N_\mu(t) = n-1}.
      \end{equation*}
Take expectations left and right, use~\eqref{eq:74}, and  independence of $N_\lambda$ and $N_\mu$. Near the end of the computation, use \eqref{eq:71}.
  \end{hint}
    \begin{solution}
\begin{align*}\label{eq:002}
\P{N_\lambda(t) + N_\mu(t) = n} 
&= \sum_{i=0}^n \P{ N_\mu(t) = n-i}\P{N_\lambda(t)=i} \\
&= \sum_{i=0}^n \frac{(\mu t)^{n-i}}{(n-i)!} \frac{(\lambda t)^i}{i!} e^{-(\mu+\lambda)t} \\
&= e^{-(\mu+\lambda)t} \sum_{i=0}^n \frac{(\mu t)^{n-i}}{(n-i)!} \frac{(\lambda t)^i}{i!}  \\
&= e^{-(\mu+\lambda)t}\frac 1{n!} \sum_{i=0}^n {n \choose i }(\mu t)^{n-i}(\lambda t)^i, \quad\text{ (binomial formula)}   \\
&= \frac{((\mu+\lambda)t)^n}{n!}e^{-(\mu+\lambda)t}.
  \end{align*}
    \end{solution}
\end{exercise}

\begin{exercise} [\faCalculator]
  If the Poisson arrival processes $N_\lambda$ and $N_\mu$  are independent,
use moment-generating functions to show that
$N_\lambda + N_\mu$ is  a Poisson process with rate $(\lambda + \mu)t$. 
\begin{hint}
  Use~\eqref{eq:73} and~\eqref{eq:75}.
\end{hint}

\begin{solution}
\begin{align*}
M_{N_\lambda(t)+N_\mu(t)}(s) 
&= M_{N_\lambda(t)}(s)\cdot M_{N_{\mu}(t)}(s) \\
&=\exp(\lambda t (e^s -1))\cdot \exp(\mu t(e^s-1)) \\
&= \exp((\lambda + \mu)t (e^s-1)).
\end{align*}
The last expression is the moment-generating function of a Poisson random variable  with parameter $(\lambda+\mu)t$.
    \end{solution}
\end{exercise}



\begin{exercise}[\faCalculator]
If the Poisson arrival processes $N_\lambda$ and $N_\mu$  are independent, show 
that 
 \begin{equation*}
    \P{N_\lambda(h) = 1 | N_\lambda(h) + N_\mu(h) = 1} =
\frac{\lambda}{\lambda+\mu}.
    \end{equation*}
    Note that the right-hand-side does not depend on $h$, hence it
    holds for any time $h$, whether it is small or not.  
    \begin{hint}
Use the standard formula for conditional probability and that  $N_\lambda(t) + N_\mu(t) \sim \text{P}((\lambda + \mu)t)$. Interpret the result.
% Suppose
%       we write $N(t)=N_\lambda(t) + N_\mu(t)$. Then
%       \begin{equation*}
%       \P{N_\lambda(h) = 1| N(h) = 1}
%       \end{equation*}
%       is the probability that $N_\lambda(h)=1$ and $N_\mu(h)=0$ \emph{given}
%       that $N(t)=1$. Use the standard formula for conditional probability. 
    \end{hint}
    \begin{solution}
  With the above:
  \begin{align*}
&    \P{N_\lambda(h) = 1 | N_\lambda(h) + N_\mu(h) = 1} \\
&= \frac{\P{N_\lambda(h) = 1,  N_\lambda(h) + N_\mu(h) = 1} }{ \P{N_\lambda(h) + N_\mu(h) = 1}} \\ 
&= \frac{\P{N_\lambda(h) = 1,  N_\mu(h) = 0}}{ \P{N_{\lambda+\mu}(h) = 1}} \\ 
&= \frac{\P{N_\lambda(h) = 1}\P{N_\mu(h) = 0}}{ \P{N_{\lambda+\mu}(h)  = 1}} \\ 
&= \frac{\lambda h \exp(-\lambda h) \exp(-\mu h)}{((\lambda+\mu)h)\exp{(-(\lambda+\mu)h)}}\\
&= \frac{\lambda h \exp{(-(\lambda + \mu)h)}}{((\lambda+\mu)h)\exp{(-(\lambda+\mu)h)}}\\
&= \frac{\lambda}{\lambda+\mu}.
  \end{align*}

  Given that a customer arrived in $[0,t]$, the probability that it is of the first type is $\lambda/(\lambda+\mu)$. 
    \end{solution}
\end{exercise}


Besides merging Poisson streams, we can also consider the concept of \emph{splitting}, or \emph{thinning}, a stream into sub-streams, as follows.
% When a customer arrives, throw a coin that lands heads with probability $p$ and tails with $q=1-p$.
% When the coin lands heads, the customer is of type 1 (e.g., a woman), otherwise of type 2 (e.g., a child).
Model the stream of people passing by a shop as a Poisson process $N_\lambda$. In the figure below these arrivals are marked as crosses at the upper line. 
With probability $p$ a person decides, independent of anything else, to enter the shop; the crosses at the lower line are the customers that enter the shop.
In the figure, the Bernoulli random variable $B_1=1$ so that the first passerby enters the shop; the second passerby does not enter as $B_2=0$, and so on.

  \begin{center}
\begin{tikzpicture}[scale=1]
%\draw[[-{Triangle[open]},dotted] (0,10)--(8.5,10);
\draw[->] (0,2)--(10,2);
\node[left] at (0,2) {$N_\lambda$};
%\draw[->] (0,1)--(10,1);
%\node[left] at (0,1) {$B_k$};
\draw[->] (0,0)--(10,0);
\node[left] at (0,0) {$N_{\lambda p}$};

\draw[{Rays[]}-{Rays[]},dotted] (1,2.06)--(1,-0.06) 
node[below]  {$B_1=1$};

\draw[{Rays[]}-{Circle[open]},dotted] (2.5,2.06)--(2.5,1.3) 
node[below] {$B_2=0$};

\draw[{Rays[]}-{Circle[open]},dotted] (4,2.06)--(4,1.3) 
node[below, fill=white] {$B_3=0$};

\draw[{Rays[]}-{Rays[]},dotted] (5,2.06)--(5,-0.06) 
node[below]  {$B_4=1$};

\draw[{Rays[]}-{Rays[]},dotted] (6.5,2.06)--(6.5,-0.06) 
node[below]  {$B_5=1$};


\draw[{Rays[]}-{Circle[open]},dotted] (7.5,2.06)--(7.5,1.3) 
node[below, fill=white] {$B_6=0$};

\end{tikzpicture}
  \end{center}


\begin{exercise}[\faCalculator]\label{ex:1}
  Show with conditioning that thinning the Poisson process $N_\lambda$ by means of Bernoulli random variables with success probability $p$ results in a Poisson process $N_{\lambda p}$.
  \begin{hint}
Suppose that  $N_1$ is the  thinned stream, and $N$ the original stream.  Condition on the total number of arrivals $N(t)=n$ up to time
      $t$. Then, realize that the probability that a person is of type 1 is $p$. Hence when you consider $n$ people in
      total, the number $N_1(t)$ of type 1 people is binomially distributed. Thus, given that $n$ people arrived, the probability of $k$ `successes' (i.e., arrivals of type 1), is 
      \begin{equation*}
        \P{N_1(t)=k \given N(t) = n} = {n \choose k} p^k (1-p)^{n-k}.
      \end{equation*}
Use~\eqref{eq:70} to decompose the $\{N_1=k\}$, and \eqref{eq:76} at the end. 
\end{hint}
\begin{solution}
\begin{align*}
    \P{N_1(t) = k}
&= \sum_{n=k}^\infty \P{N_1(t) =k, N(t) = n} 
= \sum_{n=k}^\infty \P{N_1(t) =k\given N(t) = n}\P{N(t)=n} \\
&= \sum_{n=k}^\infty \P{N_1(t) =k\given N(t) = n}e^{-\lambda t} \frac{(\lambda t)^n}{n!}\\
&= \sum_{n=k}^\infty {n \choose k} p^k (1-p)^{n-k} e^{-\lambda t} \frac{(\lambda t)^n}{n!}, \quad\text{by the hint}\\
&= e^{-\lambda t}\sum_{n=k}^\infty  \frac{p^k (1-p)^{n-k}}{k! (n-k)!} (\lambda t)^n
= e^{-\lambda t} \frac{(\lambda t p)^k}{k!} \sum_{n=k}^\infty  \frac{(\lambda t (1-p))^{n-k}}{(n-k)!}\\
&= e^{-\lambda t} \frac{(\lambda  t p)^k}{k!} \sum_{n=0}^\infty  \frac{(\lambda t (1-p))^{n}}{n!}
= e^{-\lambda t} \frac{(\lambda t p)^k}{k!} e^{\lambda t(1-p)} \\
&= e^{-\lambda t p} \frac{(\lambda t p)^k}{k!}.
\end{align*}
\end{solution}
\end{exercise}    


\begin{exercise}[\faCalculator]\label{ex:18888}
Show with moment-generation functions 
that thinning the Poisson process $N_\lambda$ by means of Bernoulli random variables with success probability $p$ results in a Poisson process $N_{\lambda p}$.
  \begin{hint}
Dropping the dependence of $N$ on $t$ for the moment for notational convenience,  consider the random variable 
  \begin{equation*}
    Y = \sum_{i=1}^N Z_i,
  \end{equation*}
  with $N\sim P(\lambda)$ and $Z_i\sim B(p)$. Show that the moment-generating function of $Y$ is equal to the moment-generating
  function of a Poisson random variable with parameter $\lambda p$.
  \end{hint}
    \begin{solution}
Consider $Y=\sum_{i=1}^N Z_i$. Suppose that $N=n$, so that $n$
arrivals occurred. Then we throw $n$ independent coins with success probability
$p$. It is clear that $Y$ is indeed a thinned Poisson random variable.

Model the coins as a generic Bernoulli distributed random variable
$Z$.  We first need
\begin{equation*}
  \E{e^{s Z}} = e^0 \P{Z=0} + e^{s} \P{Z=1} = (1-p) + e^s p.
\end{equation*}
Suppose that $N=n$, then since the outcomes $Z_i$ of the coins are i.i.d.,
\begin{equation*}
\E{e^{s\sum_{i=1}^n Z_i}} = \left(\E{e^{s Z}}\right)^n = \left(1 + p (e^s - 1)\right)^n,
\end{equation*}
where we use \eqref{eq:73}. 

With  \eqref{eq:77}, 
\begin{align*}
  \E{e^{s Y}}
&= \E{\sum_{n=0}^\infty e^{s\sum_{i=1}^N Z_i} \1{N=n}} \\
&= \E{\sum_{n=0}^\infty e^{s\sum_{i=1}^n Z_i} \1{N=n}} \\
&= \sum_{n=0}^\infty \E{e^{s\sum_{i=1}^n Z_i} \1{N=n}} \\
&= \sum_{n=0}^\infty \E{e^{s\sum_{i=1}^n Z_i}} \E{\1{N=n}}, \text{ by independence of $Z_i$ and $N$}, \\
&= \sum_{n=0}^\infty \left(1+p(e^s-1)\right)^n \P{N=n} \\
&= \sum_{n=0}^\infty \left(1+p(e^s-1)\right)^n e^{-\lambda} \frac{\lambda^n}{n!}
= e^{-\lambda} \sum_{n=0}^\infty \frac{\left(1+p(e^s-1)\right)^n \lambda^n}{n!}\\
&= e^{-\lambda} \exp(\lambda (1+p(e^s-1))) = \exp(\lambda p (e^s - 1)).
\end{align*}
% Thus, $Y$ has the same moment-generating function as a Poisson
% distributed random variable with parameter $\lambda p$. Since
% the moment-generating function specifies the distribution uniquely,
% $Y\sim P(\lambda p)$.
\end{solution}
\end{exercise}    

The concepts of merging and thinning are useful to analyze queueing networks.
Suppose the departure stream of a machine splits into two sub-streams, e.g., a fraction $p$ of the jobs moves on to another machine and the rest ($1-p$) of the jobs leaves the system.
Then we can model the arrival stream at the second machine as a thinned stream (with probability $p$) of the departures of the first machine.
Merging occurs where the output streams of various stations arrive at another station.

\begin{exercise}\faFlask
Use the solution of Exercise~\ref{ex:18888} to prove the result of Exercise~\ref{ex:31}. 
\begin{solution}
Take $Y=\sum_{i=1}^n Z_i$ with $Z_i\sim B(p)$. Then, 
\begin{equation*}
M_Y(s) = \E{e^{s\sum_{i=1}^n Z_i}} = \left(\E{e^{s Z}}\right)^n = \left(1 + p (e^s - 1)\right)^n. 
\end{equation*}
Recall that $p= \lambda t/ n$. Then, with \eqref{eq:65},
\begin{equation*}
\lim_{n\to\infty}  \left(1 + \frac{\lambda t}{n} (e^s - 1)\right)^n = \exp{\lambda t (e^s-1)}. 
\end{equation*}

\end{solution}

\end{exercise}


\Closesolutionfile{hint}
\Closesolutionfile{ans}

\opt{solutionfiles}{
\subsection*{Hints}
\input{hint}
\subsection*{Solutions}
\input{ans}
}

%\clearpage

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../queueing_book"
%%% End:
